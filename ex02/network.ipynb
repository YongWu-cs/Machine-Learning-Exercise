{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2\n",
    "Yong Wu</br>\n",
    "ZiWei Liu</br>\n",
    "WenZhuo Chen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Programming a nural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLULayer(object):\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the ReLU of the input\n",
    "        relu = np.maximum(0,input) # your code here\n",
    "        return relu\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of ReLU from upstream_gradient and the stored input\n",
    "        upstream_gradient[np.maximum(0,self.input)<=0]=0\n",
    "        downstream_gradient =upstream_gradient # your code here\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # ReLU is parameter-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(object):\n",
    "    def __init__(self, n_classes):\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # return the softmax of the input\n",
    "        softmax = np.exp(self.input)/np.sum(np.exp(self.input),axis=1).reshape(-1,1) # your code here\n",
    "        return softmax\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_labels):\n",
    "        # return the loss derivative with respect to the stored inputs\n",
    "        # (use cross-entropy loss and the chain rule for softmax,\n",
    "        #  as derived in the lecture)\n",
    "        one_hot_labels = np.eye(self.n_classes)[true_labels]\n",
    "        downstream_gradient = (predicted_posteriors - one_hot_labels) / predicted_posteriors.shape[0]\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        pass # softmax is parameter-free"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearLayer(object):\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        self.n_inputs  = n_inputs\n",
    "        self.n_outputs = n_outputs\n",
    "        # randomly initialize weights and intercepts\n",
    "        self.B = np.random.normal(size=[self.n_inputs,self.n_outputs]) # your code here\n",
    "        self.b = np.random.normal(size=[1,self.n_outputs]) # your code here\n",
    "\n",
    "    def forward(self, input):\n",
    "        # remember the input for later backpropagation\n",
    "        self.input = input\n",
    "        # compute the scalar product of input and weights\n",
    "        # (these are the preactivations for the subsequent non-linear layer)\n",
    "        preactivations =self.input.dot(self.B)+self.b # your code here\n",
    "        return preactivations\n",
    "\n",
    "    def backward(self, upstream_gradient):\n",
    "        # compute the derivative of the weights from\n",
    "        # upstream_gradient and the stored input\n",
    "        self.grad_b = np.sum(upstream_gradient,axis=0) # your code here\n",
    "        self.grad_B = self.input.T.dot(upstream_gradient) # your code here\n",
    "        # compute the downstream gradient to be passed to the preceding layer\n",
    "        downstream_gradient = upstream_gradient.dot(self.B.T) # your code here\n",
    "        return downstream_gradient\n",
    "\n",
    "    def update(self, learning_rate):\n",
    "        # update the weights by batch gradient descent\n",
    "        self.B = self.B - learning_rate * self.grad_B\n",
    "        self.b = self.b - learning_rate * self.grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, n_features, layer_sizes):\n",
    "        # constuct a multi-layer perceptron\n",
    "        # with ReLU activation in the hidden layers and softmax output\n",
    "        # (i.e. it predicts the posterior probability of a classification problem)\n",
    "        #\n",
    "        # n_features: number of inputs\n",
    "        # len(layer_size): number of layers\n",
    "        # layer_size[k]: number of neurons in layer k\n",
    "        # (specifically: layer_sizes[-1] is the number of classes)\n",
    "        self.n_layers = len(layer_sizes)\n",
    "        self.layers   = []\n",
    "\n",
    "        # create interior layers (linear + ReLU)\n",
    "        n_in = n_features\n",
    "        for n_out in layer_sizes[:-1]:\n",
    "            self.layers.append(LinearLayer(n_in, n_out))\n",
    "            self.layers.append(ReLULayer())\n",
    "            n_in = n_out\n",
    "\n",
    "        # create last linear layer + output layer\n",
    "        n_out = layer_sizes[-1]\n",
    "        self.layers.append(LinearLayer(n_in, n_out))\n",
    "        self.layers.append(OutputLayer(n_out))\n",
    "\n",
    "    def forward(self, X):\n",
    "        # X is a mini-batch of instances\n",
    "        batch_size = X.shape[0]\n",
    "        # flatten the other dimensions of X (in case instances are images)\n",
    "        X = X.reshape(batch_size, -1)\n",
    "\n",
    "        # compute the forward pass\n",
    "        # (implicitly stores internal activations for later backpropagation)\n",
    "        result = X\n",
    "        for layer in self.layers:\n",
    "            result = layer.forward(result)\n",
    "        return result\n",
    "\n",
    "    def backward(self, predicted_posteriors, true_classes):\n",
    "        # perform backpropagation w.r.t. the prediction for the latest mini-batch X\n",
    "        gradient=self.layers[-1].backward(predicted_posteriors , true_classes) # your cod0e here\n",
    "        for i in range(len(self.layers)-2,-1,-1):\n",
    "            gradient=self.layers[i].backward(gradient)\n",
    "\n",
    "    def update(self, X, Y, learning_rate):\n",
    "        posteriors = self.forward(X)\n",
    "        self.backward(posteriors, Y)\n",
    "        for layer in self.layers:\n",
    "            layer.update(learning_rate)\n",
    "\n",
    "    def train(self, x, y, n_epochs, batch_size, learning_rate):\n",
    "        N = len(x)\n",
    "        n_batches = N // batch_size\n",
    "        for i in range(n_epochs):\n",
    "            # print(\"Epoch\", i)\n",
    "            # reorder data for every epoch\n",
    "            # (i.e. sample mini-batches without replacement)\n",
    "            permutation = np.random.permutation(N)\n",
    "\n",
    "            for batch in range(n_batches):\n",
    "                # create mini-batch\n",
    "                start = batch * batch_size\n",
    "                x_batch = x[permutation[start:start+batch_size]]\n",
    "                y_batch = y[permutation[start:start+batch_size]]\n",
    "\n",
    "                # perform one forward and backward pass and update network parameters\n",
    "                self.update(x_batch, y_batch, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validation(N,folds,n_epochs,batch_size,learning_rate):\n",
    "    n_features = 2\n",
    "    n_classes  = 2\n",
    "    skf = StratifiedKFold(n_splits=folds)  #cross-validation\n",
    "\n",
    "    error_rate=[0,0,0,0]\n",
    "    X_all, Y_all = datasets.make_moons(N, noise=0.05)\n",
    "    for train, test in skf.split(X_all, Y_all):\n",
    "        X_train,Y_train=X_all[train],Y_all[train]\n",
    "        X_test,Y_test=X_all[test],Y_all[test]\n",
    "\n",
    "        # standardize features to be in [-1, 1]\n",
    "        offset  = X_train.min(axis=0)\n",
    "        scaling = X_train.max(axis=0) - offset\n",
    "        X_train = ((X_train - offset) / scaling - 0.5) * 2.0\n",
    "        X_test  = ((X_test  - offset) / scaling - 0.5) * 2.0\n",
    "       \n",
    "        # set hyperparameters (play with these!)\n",
    "        layer_sizes_22 = [2, 2, n_classes]\n",
    "        layer_sizes_33 = [3, 3, n_classes]\n",
    "        layer_sizes_55 = [5, 5, n_classes]\n",
    "        layer_sizes_3030 = [30, 30, n_classes]\n",
    "\n",
    "        # create network\n",
    "        network_22 = MLP(n_features, layer_sizes_22)\n",
    "        network_33 = MLP(n_features, layer_sizes_33)\n",
    "        network_55 = MLP(n_features, layer_sizes_55)\n",
    "        network_3030 = MLP(n_features, layer_sizes_3030)\n",
    "\n",
    "        # train\n",
    "        network_22.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "        network_33.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "        network_55.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "        network_3030.train(X_train, Y_train, n_epochs, batch_size, learning_rate)\n",
    "\n",
    "        # test\n",
    "        # determine class predictions from posteriors by winner-takes-all rule\n",
    "        # compute and output the error rate of predicted_classes\n",
    "        predicted_posteriors_22 = network_22.forward(X_test)\n",
    "        predicted_classes_22 = np.argmax(predicted_posteriors_22,axis=1) \n",
    "        error_rate[0] =error_rate[0]+ np.sum(predicted_classes_22!=Y_test)/len(Y_test)\n",
    "        \n",
    "        predicted_posteriors_33 = network_33.forward(X_test)\n",
    "        predicted_classes_33 = np.argmax(predicted_posteriors_33,axis=1) \n",
    "        error_rate[1] =error_rate[1]+ np.sum(predicted_classes_33!=Y_test)/len(Y_test)\n",
    "\n",
    "        predicted_posteriors_55 = network_55.forward(X_test)\n",
    "        predicted_classes_55 = np.argmax(predicted_posteriors_55,axis=1) \n",
    "        error_rate[2] =error_rate[2]+ np.sum(predicted_classes_55!=Y_test)/len(Y_test)\n",
    "\n",
    "        predicted_posteriors_3030 = network_3030.forward(X_test)\n",
    "        predicted_classes_3030 = np.argmax(predicted_posteriors_3030,axis=1) \n",
    "        error_rate[3] =error_rate[3]+ np.sum(predicted_classes_3030!=Y_test)/len(Y_test)\n",
    "\n",
    "    label_name={0:\"2*2\",1:\"3*3\",2:\"5*5\",3:\"30*30\"}\n",
    "    for i in range(len(error_rate)):\n",
    "        error_rate[i]=error_rate[i]/folds\n",
    "        print(\"{} MLP network error rate is:{}\".format(label_name[i],error_rate[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='MLP network')\n",
    "    \n",
    "    parser.add_argument('--dataset-num', type=int, default=4000, metavar='N',\n",
    "                        help='input dataset num for dataset (default: 4000)')\n",
    "    parser.add_argument('--batch-size', type=int, default=200, metavar='N',\n",
    "                        help='input batch size for training (default: 200)')\n",
    "    parser.add_argument('--epochs', type=int, default=5, metavar='N',\n",
    "                        help='number of epochs to train (default: 5)')\n",
    "    parser.add_argument('--lr', type=float, default=0.05, metavar='LR',\n",
    "                        help='learning rate (default: 0.05)')\n",
    "    parser.add_argument('--fold', type=int, default=5, metavar='N',\n",
    "                        help='input k-folds for cross-validation (default: 5)')\n",
    "    args = parser.parse_args(args=[])\n",
    "\n",
    "    cross_validation(args.dataset_num,args.fold,args.epochs,args.batch_size,args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2*2 MLP network error rate is:0.31625000000000003\n",
      "3*3 MLP network error rate is:0.23775\n",
      "5*5 MLP network error rate is:0.12075\n",
      "30*30 MLP network error rate is:0.0017500000000000003\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
