{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45df74f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from sympy.tensor import tensor\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.nn.functional import conv2d, max_pool2d, cross_entropy\n",
    "\n",
    "plt.rc(\"figure\", dpi=100)\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "# transform images into normalized tensors\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=True,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    \"./\",\n",
    "    download=True,\n",
    "    train=False,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=1,\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "def init_weights(shape):\n",
    "    # Kaiming He initialization (a good initialization is important)\n",
    "    # https://arxiv.org/abs/1502.01852\n",
    "    std = np.sqrt(2. / shape[0])\n",
    "    w = torch.randn(size=shape) * std\n",
    "    w.requires_grad = True\n",
    "    return w\n",
    "\n",
    "\n",
    "def rectify(x):\n",
    "    # Rectified Linear Unit (ReLU)\n",
    "    return torch.max(torch.zeros_like(x), x)\n",
    "\n",
    "class RMSprop(optim.Optimizer):\n",
    "    \"\"\"\n",
    "    This is a reduced version of the PyTorch internal RMSprop optimizer\n",
    "    It serves here as an example\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, eps=1e-8):\n",
    "        defaults = dict(lr=lr, alpha=alpha, eps=eps)\n",
    "        super(RMSprop, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self):\n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                # state initialization\n",
    "                if len(state) == 0:\n",
    "                    state['square_avg'] = torch.zeros_like(p.data)\n",
    "\n",
    "                square_avg = state['square_avg']\n",
    "                alpha = group['alpha']\n",
    "\n",
    "                # update running averages\n",
    "                square_avg.mul_(alpha).addcmul_(grad, grad, value=1 - alpha)\n",
    "                avg = square_avg.sqrt().add_(group['eps'])\n",
    "\n",
    "                # gradient update\n",
    "                p.data.addcdiv_(grad, avg, value=-group['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ab4bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def dropout(X, p_drop=0.5):\n",
    "    mask = np.random.binomial(1, 1 - p_drop, size=X.shape) / (1 - p_drop)\n",
    "    return X * torch.tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b8b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "def PRelu(X, a):\n",
    "    return torch.where(X <= 0, a * X, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb47cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_model(X, w_h, w_h2, w_o, p_drop_input, p_drop_hidden, a):\n",
    "    # Dropout at the input layer\n",
    "    X = dropout(X, p_drop=p_drop_input)\n",
    "    X = X.to(w_h.dtype)  # Convert X to the same data type as w_h\n",
    "    h = X @ w_h\n",
    "    # a1 = torch.nn.Parameter(torch.zeros_like(h), requires_grad=True)\n",
    "    # Forward pass through the first hidden layer\n",
    "    h = PRelu(h, a)\n",
    "    # Dropout at the first hidden layer\n",
    "    h = dropout(h, p_drop=p_drop_hidden)\n",
    "    # Forward pass through the second hidden layer\n",
    "    h2 = h @ w_h2\n",
    "    # a2 = torch.nn.Parameter(torch.zeros_like(h2), requires_grad=True)\n",
    "    h2 = PRelu(h2, a)\n",
    "    # Dropout at the second hidden layer\n",
    "    h2 = dropout(h2, p_drop=p_drop_hidden)\n",
    "    # Forward pass through the output layer\n",
    "    output = h2 @ w_o\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b10d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # initialize weights\n",
    "\n",
    "    # input shape is (B, 784)\n",
    "    w_h = init_weights((784, 625))\n",
    "    # hidden layer with 625 neurons\n",
    "    w_h2 = init_weights((625, 625))\n",
    "    # hidden layer with 625 neurons\n",
    "    w_o = init_weights((625, 10))\n",
    "    a = init_weights((1, 625))\n",
    "    # output shape is (B, 10)\n",
    "\n",
    "    optimizer = RMSprop(params=[w_h, w_h2, w_o, a])\n",
    "\n",
    "    n_epochs = 100\n",
    "\n",
    "    train_loss = []\n",
    "    test_loss = []\n",
    "\n",
    "    # put this into a training loop over 100 epochs\n",
    "    for epoch in range(n_epochs + 1):\n",
    "        train_loss_this_epoch = []\n",
    "        for idx, batch in enumerate(train_dataloader):\n",
    "            x, y = batch\n",
    "\n",
    "            # our model requires flattened input\n",
    "            x = x.reshape(batch_size, 784)\n",
    "            # feed input through model\n",
    "            # noise_py_x = model(x, w_h, w_h2, w_o)\n",
    "            # noise_py_x = dropout_model(x, w_h, w_h2, w_o, 0.5, 0.5)\n",
    "            noise_py_x = dropout_model(x, w_h, w_h2, w_o, 0.5, 0.5, a)\n",
    "\n",
    "            # reset the gradient\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # the cross-entropy loss function already contains the softmax\n",
    "            loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "\n",
    "            train_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            # compute the gradient\n",
    "            loss.backward()\n",
    "            # update weights\n",
    "            optimizer.step()\n",
    "\n",
    "        train_loss.append(np.mean(train_loss_this_epoch))\n",
    "\n",
    "        # test periodically\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch: {epoch}\")\n",
    "            print(f\"Mean Train Loss: {train_loss[-1]:.2e}\")\n",
    "            test_loss_this_epoch = []\n",
    "\n",
    "            # no need to compute gradients for validation\n",
    "            with torch.no_grad():\n",
    "                for idx, batch in enumerate(test_dataloader):\n",
    "                    x, y = batch\n",
    "                    x = x.reshape(batch_size, 784)\n",
    "                    noise_py_x = dropout_model(x, w_h, w_h2, w_o, 0.5, 0.5, a)\n",
    "\n",
    "                    loss = cross_entropy(noise_py_x, y, reduction=\"mean\")\n",
    "                    test_loss_this_epoch.append(float(loss))\n",
    "\n",
    "            test_loss.append(np.mean(test_loss_this_epoch))\n",
    "\n",
    "            print(f\"Mean Test Loss:  {test_loss[-1]:.2e}\")\n",
    "\n",
    "    plt.plot(np.arange(n_epochs + 1), train_loss, label=\"Train\")\n",
    "    plt.plot(np.arange(1, n_epochs + 2, 10), test_loss, label=\"Test\")\n",
    "    plt.title(\"Train and Test Loss over Training\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.savefig('./loss.png')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:PyTorch] *",
   "language": "python",
   "name": "conda-env-PyTorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
